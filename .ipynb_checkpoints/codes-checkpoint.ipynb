{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data, convert categorical variables to numeric variables, split data set into train and \n",
    "# test. Those steps are the same as the steps we took before in the decision tree repository.\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "names = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \n",
    "         \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \n",
    "         \"hours_per_week\", \"native_country\", \"high_income\"]\n",
    "\n",
    "income = pandas.read_csv(\"income.csv\", header=None, index_col=False, names=names)\n",
    "\n",
    "# Convert the categorical variables to numeric variables\n",
    "convert_list = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', \n",
    "                'race', 'sex', 'native_country', 'high_income']\n",
    "for column in convert_list:\n",
    "    col = pandas.Categorical.from_array(income[column])\n",
    "    income[column] = col.codes\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\",\n",
    "           \"race\", \"sex\", \"hours_per_week\", \"native_country\"]\n",
    "\n",
    "income = income.reindex(numpy.random.permutation(income.index))\n",
    "\n",
    "train_max_row = math.floor(income.shape[0] * .7)\n",
    "train = income.iloc[:train_max_row]\n",
    "test = income.iloc[train_max_row:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first try to create two decision trees with slightly different parameters, and check their accuracy separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.786587913238\n",
      "0.774814226817\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=75)\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "clf2 = DecisionTreeClassifier(random_state=1, max_depth=6)\n",
    "clf2.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predict_1 = clf.predict(test[columns])\n",
    "auc_1 = roc_auc_score(predict_1, test['high_income'])\n",
    "\n",
    "predict_2 = clf2.predict(test[columns])\n",
    "auc_2 = roc_auc_score(predict_2, test['high_income'])\n",
    "\n",
    "print(auc_1)\n",
    "print(auc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794347522655\n"
     ]
    }
   ],
   "source": [
    "# Combining predictions\n",
    "predictions = clf.predict_proba(test[columns])[:,1]\n",
    "predictions2 = clf2.predict_proba(test[columns])[:,1]\n",
    "\n",
    "auc = roc_auc_score(numpy.round((predictions + predictions2) / 2), test['high_income'])\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the combined predictions of the two trees are more accurate than any single tree.\n",
    "\n",
    "Next, we will introduce variations to decision trees using the Bagging technique, which means sampling with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785941008368\n"
     ]
    }
   ],
   "source": [
    "# We'll build 10 trees using Bagging\n",
    "tree_count = 10\n",
    "\n",
    "# Each bag will have 70% of the number of original rows.\n",
    "bag_proportion = .7\n",
    "\n",
    "predictions = []\n",
    "for i in range(tree_count):\n",
    "    \n",
    "    # We set random state to i instead of a fixed value so we don't get the same sample every \n",
    "    # loop. That would make all of our trees the same.\n",
    "    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)\n",
    "    \n",
    "    # Fit a decision tree model to the \"bag\".\n",
    "    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=75)\n",
    "    clf.fit(bag[columns], bag[\"high_income\"])\n",
    "    \n",
    "    # Make predictions on the test data.\n",
    "    predictions.append(clf.predict_proba(test[columns])[:,1])\n",
    "\n",
    "rounded = numpy.round(sum(predictions)/len(predictions))\n",
    "auc = roc_auc_score(rounded, test['high_income'])\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Again we'll use calc_entropy and calc_information_gain we defined earlier in decision tree \n",
    "# repository to calculate entropy and information gain.\n",
    "\n",
    "# Calculate entropy given a pandas Series, list, or numpy array.\n",
    "def calc_entropy(column):\n",
    "\n",
    "    counts = numpy.bincount(column)\n",
    "    probabilities = counts / len(column)\n",
    "    \n",
    "    entropy = 0\n",
    "    for prob in probabilities:\n",
    "        if prob > 0: \n",
    "            entropy += prob * math.log(prob, 2)\n",
    "    return -entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate information gain given a dataset, column to split on, and target.\n",
    "def calc_information_gain(data_set, split_name, target_name):\n",
    "    \n",
    "    median = numpy.median(data_set[split_name])\n",
    "    left_split = data_set[data_set[split_name] <= median]\n",
    "    right_split = data_set[data_set[split_name] > median]\n",
    "    \n",
    "    left_entropy = calc_entropy(left_split[target_name])\n",
    "    right_entropy = calc_entropy(right_split[target_name])\n",
    "    total_entropy = calc_entropy(data_set[target_name])\n",
    "    \n",
    "    information_gain = total_entropy - (len(left_split) / len(data_set) * left_entropy +\n",
    "                                       len(right_split) / len(data_set) * right_entropy)\n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify find_best_column function we used before in decision tree repository to select a random sample from columns before computing information gain. Each subset will have 2 items in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'median': 4.5, 'column': 'employment', 'number': 1, 'left': {'median': 25.0, 'column': 'age', 'number': 2, 'left': {'median': 22.5, 'column': 'age', 'number': 3, 'left': {'label': 0, 'number': 4}, 'right': {'label': 1, 'number': 5}}, 'right': {'label': 0, 'number': 6}}, 'right': {'median': 40.0, 'column': 'age', 'number': 7, 'left': {'median': 37.5, 'column': 'age', 'number': 8, 'left': {'label': 1, 'number': 9}, 'right': {'label': 0, 'number': 10}}, 'right': {'label': 1, 'number': 11}}}\n",
      "{'median': 37.5, 'column': 'age', 'number': 12, 'left': {'median': 4.0, 'column': 'employment', 'number': 13, 'left': {'median': 22.5, 'column': 'age', 'number': 14, 'left': {'label': 0, 'number': 15}, 'right': {'label': 1, 'number': 16}}, 'right': {'label': 1, 'number': 17}}, 'right': {'median': 55.0, 'column': 'age', 'number': 18, 'left': {'median': 47.5, 'column': 'age', 'number': 19, 'left': {'label': 0, 'number': 20}, 'right': {'label': 1, 'number': 21}}, 'right': {'label': 0, 'number': 22}}}\n"
     ]
    }
   ],
   "source": [
    "# Select random features\n",
    "# Create a model data set\n",
    "\n",
    "data = pandas.DataFrame([\n",
    "    [0,4,20,0],\n",
    "    [0,4,60,2],\n",
    "    [0,5,40,1],\n",
    "    [1,4,25,1],\n",
    "    [1,5,35,2],\n",
    "    [1,5,55,1]\n",
    "    ])\n",
    "data.columns = [\"high_income\", \"employment\", \"age\", \"marital_status\"]\n",
    "\n",
    "# Set a random seed to make results reproducible.\n",
    "numpy.random.seed(1)\n",
    "\n",
    "# The dictionary to store our tree.\n",
    "tree = {}\n",
    "nodes = []\n",
    "\n",
    "# The function to find the column to split on.\n",
    "def find_best_column(data, target_name, columns):\n",
    "    information_gains = []\n",
    "    \n",
    "    for col in columns:\n",
    "        information_gain = calc_information_gain(data, col, \"high_income\")\n",
    "        information_gains.append(information_gain)\n",
    "\n",
    "    # Find the name of the column with the highest gain.\n",
    "    highest_gain_index = information_gains.index(max(information_gains))\n",
    "    highest_gain = columns[highest_gain_index]\n",
    "    return highest_gain\n",
    "\n",
    "# The function to construct an id3 decision tree.\n",
    "def id3(data, target, columns, tree):\n",
    "    unique_targets = pandas.unique(data[target])\n",
    "    nodes.append(len(nodes) + 1)\n",
    "    tree[\"number\"] = nodes[-1]\n",
    "\n",
    "    if len(unique_targets) == 1:\n",
    "        if 0 in unique_targets:\n",
    "            tree[\"label\"] = 0\n",
    "        elif 1 in unique_targets:\n",
    "            tree[\"label\"] = 1\n",
    "        return\n",
    "    \n",
    "    best_column = find_best_column(data, target, columns)\n",
    "    column_median = data[best_column].median()\n",
    "    \n",
    "    tree[\"column\"] = best_column\n",
    "    tree[\"median\"] = column_median\n",
    "    \n",
    "    left_split = data[data[best_column] <= column_median]\n",
    "    right_split = data[data[best_column] > column_median]\n",
    "    split_dict = [[\"left\", left_split], [\"right\", right_split]]\n",
    "    \n",
    "    for name, split in split_dict:\n",
    "        tree[name] = {}\n",
    "        id3(split, target, columns, tree[name])\n",
    "\n",
    "# Run the id3 algorithm on our dataset and print the resulting tree.\n",
    "id3(data, \"high_income\", [\"employment\", \"age\", \"marital_status\"], tree)\n",
    "print(tree)\n",
    "\n",
    "def find_best_column(data, target_name, columns):\n",
    "    information_gains = []\n",
    "    \n",
    "    # Select two columns randomly.\n",
    "    cols = numpy.random.choice(columns, 2)\n",
    "    \n",
    "    for col in cols:\n",
    "        information_gain = calc_information_gain(data, col, \"high_income\")\n",
    "        information_gains.append(information_gain)\n",
    "\n",
    "    highest_gain_index = information_gains.index(max(information_gains))\n",
    "    \n",
    "    # Get the highest gain by indexing cols.\n",
    "    highest_gain = cols[highest_gain_index]\n",
    "    \n",
    "    return highest_gain\n",
    "\n",
    "id3(data, \"high_income\", [\"employment\", \"age\", \"marital_status\"], tree)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use scikit-learn library to repeat our random subset selection process. This is easier and with far less typing. We just set the splitter parameter on DecisionTreeClassifier to \"random\", and the max_features parameter to \"auto\". If we have N columns, this will pick a subset of features of size √N, compute the gini coefficient (similar to information gain) for each, and split the node on the best column in the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.779318823421\n"
     ]
    }
   ],
   "source": [
    "# We'll build 10 trees\n",
    "tree_count = 10\n",
    "\n",
    "# Each bag will have 70% of the number of original rows.\n",
    "bag_proportion = .7\n",
    "\n",
    "predictions = []\n",
    "for i in range(tree_count):\n",
    "\n",
    "    bag = train.sample(frac=bag_proportion, replace=True, random_state=i)\n",
    "    \n",
    "    # Fit a decision tree model to the \"bag\".\n",
    "    clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=75, splitter='random', \n",
    "                                 max_features='auto')\n",
    "    clf.fit(bag[columns], bag[\"high_income\"])\n",
    "    \n",
    "    # Using the model, make predictions on the test data.\n",
    "    predictions.append(clf.predict_proba(test[columns])[:,1])\n",
    "\n",
    "combined = numpy.sum(predictions, axis=0) / 10\n",
    "rounded = numpy.round(combined)\n",
    "\n",
    "print(roc_auc_score(rounded, test[\"high_income\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify this process further by using RandomForestClassifier from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79247969822\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=150, random_state=1, min_samples_leaf=75)\n",
    "\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(predictions, test[\"high_income\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's compare the accuracy of decision tree and random forest algorithms. We can conclude that random forests have a better accuracy over decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.788248611021\n",
      "0.786587913238\n",
      "0.794602753238\n",
      "0.79247969822\n"
     ]
    }
   ],
   "source": [
    "# Compare the results of decision tree and random forest\n",
    "clf = DecisionTreeClassifier(random_state=1, min_samples_leaf=75)\n",
    "\n",
    "clf.fit(train[columns], train[\"high_income\"])\n",
    "\n",
    "predictions = clf.predict(train[columns])\n",
    "print(roc_auc_score(predictions, train[\"high_income\"]))\n",
    "\n",
    "predictions = clf.predict(test[columns])\n",
    "print(roc_auc_score(predictions, test[\"high_income\"]))\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=150, random_state=1, min_samples_leaf=75)\n",
    "\n",
    "clf.fit(train[columns], train['high_income'])\n",
    "\n",
    "predictions_train = clf.predict(train[columns])\n",
    "print(roc_auc_score(predictions_train, train['high_income']))\n",
    "\n",
    "predictions_test = clf.predict(test[columns])\n",
    "print(roc_auc_score(predictions_test, test['high_income']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
